# processar_aulas.py

import os
import json
import fitz  # PyMuPDF para leitura de PDFs
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import nltk
from nltk.tokenize import sent_tokenize
import argparse

# Diret√≥rios de entrada e sa√≠da
PASTA_AULAS   = "dados/aulas_originais"
PASTA_CHUNKS  = "dados/chunks"
PASTA_FAISS   = "dados/faiss_index"
JSON_CHUNKS   = os.path.join(PASTA_CHUNKS, "base_chunks.json")
INDEX_FAISS   = os.path.join(PASTA_FAISS,  "index.bin")
META_FAISS    = os.path.join(PASTA_FAISS,  "meta.json")

# Modelo de embedding
model = SentenceTransformer("all-MiniLM-L6-v2")  # dimens√£o t√≠pica = 384

# 1) Certificar que 'punkt' est√° dispon√≠vel
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

def extrair_texto_pdf(caminho_pdf: str) -> str:
    """
    Abre um PDF e retorna todo o texto concatenado.
    """
    texto = ""
    with fitz.open(caminho_pdf) as doc:
        for pagina in doc:
            texto += pagina.get_text()
    return texto


def dividir_em_chunks_por_sentenca(texto: str, max_tokens: int = 100) -> list[str]:
    """
    Divide o texto em chunks de at√© ~max_tokens tokens, agrupando senten√ßas inteiras.
    Usa o tokenizador de senten√ßas do NLTK (punkt).
    """
    try:
        sentencas = sent_tokenize(texto, language="portuguese")
    except LookupError:
        # Caso n√£o tenha o modelo ‚Äúportuguese‚Äù, tenta ingl√™s
        sentencas = sent_tokenize(texto, language="english")
        
    chunks = []
    buffer = []
    count = 0

    for s in sentencas:
        # Conta tokens da senten√ßa usando o tokenizer interno do SentenceTransformer
        tam = len(model.tokenizer.tokenize(s))
        if count + tam <= max_tokens:
            buffer.append(s)
            count += tam
        else:
            if buffer:
                chunks.append(" ".join(buffer))
            buffer = [s]
            count = tam

    if buffer:
        chunks.append(" ".join(buffer))

    return chunks


def contar_chunks_existentes() -> int:
    """
    Retorna quantos chunks j√° est√£o gravados em base_chunks.json.
    Se o arquivo n√£o existir, retorna 0.
    """
    if not os.path.exists(JSON_CHUNKS):
        return 0
    with open(JSON_CHUNKS, "r", encoding="utf-8") as f:
        base_chunks = json.load(f)
    return len(base_chunks)


def carregar_ou_criar_indice(dim: int):
    """
    Se j√° existir um √≠ndice FAISS em INDEX_FAISS e o JSON de chunks,
    carrega ambos. Caso contr√°rio, cria um √≠ndice IndexIVFFlat vazio,
    treina com vetores aleat√≥rios (placeholder) e salva.
    """
    os.makedirs(PASTA_CHUNKS, exist_ok=True)
    os.makedirs(PASTA_FAISS,  exist_ok=True)

    if os.path.exists(INDEX_FAISS) and os.path.exists(JSON_CHUNKS):
        # Carrega lista de chunks e √≠ndice existente
        with open(JSON_CHUNKS, "r", encoding="utf-8") as f:
            base_chunks = json.load(f)
        index = faiss.read_index(INDEX_FAISS)
        if not os.path.exists(META_FAISS):
            with open(META_FAISS, "w", encoding="utf-8") as f:
                json.dump({"full_trained": False}, f)
    else:
        # Cria do zero
        base_chunks = []
        nlist = 100  # N√∫mero de clusters para IVF (ajuste conforme volume esperado)
        quantizer = faiss.IndexFlatL2(dim)
        index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)
        # Treinar com vetores aleat√≥rios como placeholder (1000 vetores aleat√≥rios)
        placeholder = np.random.rand(1000, dim).astype("float32")
        index.train(placeholder)
        faiss.write_index(index, INDEX_FAISS)
        # Inicializa JSON vazio
        with open(JSON_CHUNKS, "w", encoding="utf-8") as f:
            json.dump(base_chunks, f, ensure_ascii=False, indent=2)
        # Cria metadata inicial
        with open(META_FAISS, "w", encoding="utf-8") as f:
            json.dump({"full_trained": False}, f)

    return base_chunks, index


def precisa_treinar_de_verdade(total_chunks: int) -> bool:
    """
    Retorna True apenas quando j√° houver pelo menos 1000 chunks e o
    √≠ndice ainda **n√£o** tiver sido treinado com embeddings reais.
    """
    meta = {"full_trained": False}
    if os.path.exists(META_FAISS):
        try:
            with open(META_FAISS, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except json.JSONDecodeError:
            meta = {"full_trained": False}
    return total_chunks >= 1000 and not meta.get("full_trained", False)


def re_treinar_indice_com_chunks_reais(base_chunks: list[dict], index: faiss.IndexIVFFlat):
    """
    Recria o √≠ndice FAISS do zero usando embeddings reais de todos os chunks.
    1) Gera embeddings em batch para cada chunk em base_chunks.
    2) Cria um novo IndexIVFFlat e chama train() com esses embeddings.
    3) Adiciona todos os embeddings ao novo √≠ndice e persiste.
    """
    textos = [c["texto"] for c in base_chunks]
    print(f"üîÑ Gerando embeddings reais para {len(textos)} chunks...")
    embeddings = model.encode(textos, batch_size=32, convert_to_numpy=True).astype("float32")

    dim = embeddings.shape[1]
    nlist = int(np.sqrt(len(embeddings))) or 1
    quantizer = faiss.IndexFlatL2(dim)
    new_index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)

    print(f"üîÑ Treinando novo √≠ndice IVF com {len(embeddings)} vetores (nlist={nlist})...")
    new_index.train(embeddings)
    new_index.add(embeddings)

    faiss.write_index(new_index, INDEX_FAISS)
    with open(META_FAISS, "w", encoding="utf-8") as f:
        json.dump({"full_trained": True}, f)
    print("‚úÖ √çndice re-treinado com embeddings reais.")
    return new_index


def limpar_chunks() -> None:
    """Remove o arquivo JSON que armazena todos os chunks."""
    if os.path.exists(JSON_CHUNKS):
        os.remove(JSON_CHUNKS)
        print("üóëÔ∏è  base_chunks.json removido.")
    else:
        print("‚ÑπÔ∏è  Nenhum base_chunks.json para remover.")


def limpar_indice() -> None:
    """Remove o √≠ndice FAISS armazenado em disco."""
    if os.path.exists(INDEX_FAISS):
        os.remove(INDEX_FAISS)
        print("üóëÔ∏è  index.bin removido.")
    else:
        print("‚ÑπÔ∏è  Nenhum index.bin para remover.")
    if os.path.exists(META_FAISS):
        os.remove(META_FAISS)


def limpar_aulas() -> None:
    """Apaga arquivos de aulas j√° enviados, mantendo o .gitkeep."""
    if not os.path.exists(PASTA_AULAS):
        return
    for nome in os.listdir(PASTA_AULAS):
        if nome == ".gitkeep":
            continue
        caminho = os.path.join(PASTA_AULAS, nome)
        if os.path.isfile(caminho):
            os.remove(caminho)
    print("üóëÔ∏è  Arquivos em aulas_originais removidos.")


def resetar_base() -> None:
    """Limpa arquivos de chunks, √≠ndice e PDFs de origem."""
    limpar_chunks()
    limpar_indice()
    limpar_aulas()
    print("‚úÖ Base de dados resetada.")


def processar_todos():
    """
    Fluxo principal:
    1) Carrega (ou cria) √≠ndice FAISS e base_chunks.
    2) Verifica novos arquivos em PASTA_AULAS que ainda n√£o foram processados.
    3) Gera chunks (por senten√ßa) para cada novo arquivo.
    4) Faz batch encode desses novos chunks e adiciona incrementalmente ao √≠ndice.
    5) Grava JSON atualizado de base_chunks e o √≠ndice FAISS.
    6) Se total_chunks >= 1000, re-treina o √≠ndice com embeddings reais de todos os chunks.
    """
    arquivos = os.listdir(PASTA_AULAS)
    if not arquivos:
        print("‚ö†Ô∏è Nenhum arquivo encontrado em dados/aulas_originais.")
        return

    # Exemplo de dimens√£o de embedding (all-MiniLM-L6-v2 ‚Üí 384)
    dim_example = 384

    # 1) Carregar ou criar √≠ndice e lista de chunks
    base_chunks, index = carregar_ou_criar_indice(dim_example)

    novos_chunks = []
    for nome_arquivo in arquivos:
        caminho = os.path.join(PASTA_AULAS, nome_arquivo)

        # S√≥ processa PDF ou TXT
        if not nome_arquivo.lower().endswith((".pdf", ".txt")):
            continue

        # Pula arquivo se j√° estiver em base_chunks (campo "origem")
        if any(c["origem"] == nome_arquivo for c in base_chunks):
            continue

        # 2) Extrair texto
        if nome_arquivo.lower().endswith(".pdf"):
            texto = extrair_texto_pdf(caminho)
        else:
            with open(caminho, "r", encoding="utf-8") as f:
                texto = f.read()

        if not texto.strip():
            print(f"‚ö†Ô∏è Arquivo vazio: {nome_arquivo}")
            continue

        # 3) Dividir em chunks por senten√ßa
        chunks_do_arquivo = dividir_em_chunks_por_sentenca(texto, max_tokens=100)
        print(f"üìÑ {nome_arquivo} ‚Üí {len(chunks_do_arquivo)} chunks")

        for i, chunk in enumerate(chunks_do_arquivo):
            novos_chunks.append({
                "id":     f"{nome_arquivo}_{i}",
                "texto":  chunk,
                "origem": nome_arquivo,
                "ordem":  i
                # Deixamos embedding vazio; ser√° gerado em batch
            })

    if not novos_chunks:
        print("‚úÖ N√£o h√° novos chunks para adicionar.")
        return

    # 4) Em batch, gerar embeddings para todos os novos chunks
    textos = [c["texto"] for c in novos_chunks]
    print(f"üîç Gerando embeddings para {len(textos)} novos chunks...")
    embeddings = model.encode(textos, batch_size=32, convert_to_numpy=True).astype("float32")

    # 5) Adicionar incrementalmente ao √≠ndice FAISS e atribuir faiss_id
    for idx, chunk_meta in enumerate(novos_chunks):
        faiss_id = index.ntotal  # √≠ndice dispon√≠vel antes de inserir
        chunk_meta["faiss_id"] = faiss_id
        index.add(embeddings[idx:idx+1])

    # 6) Atualizar JSON de chunks e persistir o √≠ndice
    base_chunks.extend(novos_chunks)
    with open(JSON_CHUNKS, "w", encoding="utf-8") as f:
        json.dump(base_chunks, f, ensure_ascii=False, indent=2)

    faiss.write_index(index, INDEX_FAISS)
    total_chunks = len(base_chunks)
    print(f"‚úÖ Adicionados {len(novos_chunks)} novos chunks. Total atual: {total_chunks} chunks.")

    # 7) Caso atinja 1000 ou mais, re-treinar com embeddings reais
    if precisa_treinar_de_verdade(total_chunks):
        # Recarrega JSON atualizado e re-treina
        with open(JSON_CHUNKS, "r", encoding="utf-8") as f:
            base_atualizado = json.load(f)
        index = re_treinar_indice_com_chunks_reais(base_atualizado, index)
    else:
        faltam = 1000 - total_chunks
        print(f"‚ÑπÔ∏è Ainda faltam {faltam} chunks para treinar com embeddings reais.")

    print("üöÄ Processamento conclu√≠do.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Processa ou limpa a base de dados")
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Remove arquivos de chunks e √≠ndice para come√ßar do zero",
    )
    args = parser.parse_args()
    if args.reset:
        resetar_base()
    else:
        processar_todos()
